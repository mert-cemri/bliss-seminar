<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Prof. Varun Jog (University of Cambridge)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Faculty</a></div>
<div class="menu-item"><a href="Alumni.html">Alumni</a></div>
<div class="menu-item"><a href="students.html">Current&nbsp;Students&nbsp;&amp;&nbsp;Postdocs</a></div>
<div class="menu-item"><a href="Seminar/index.html">BLISS&nbsp;Seminar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Prof. Varun Jog (University of Cambridge)</h1>
</div>
<p>1:00pm - 2:00pm PT Friday, October 25, 2024</p>
<h2>Title and Abstract</h2>
<p><b>Old problems, new perspectives: A fresh look at classical hypothesis testing</b><br />
<br /></p>
<p>Simple binary hypothesis testing is easily stated: Given two distributions p and q and n i.i.d. observations from either p or q, determine the distribution they come from. The problem has been studied for over a century and the optimal tests (Neyman&ndash;Pearson test and the MAP rule) and decay exponents of the errors (KL divergence and Chernoff information) are known exactly. Surprisingly, the sample complexity question &ndash; characterizing the necessary and sufficient sample size n to attain desired errors &ndash; was largely ignored in the literature. This is the first &ldquo;old problem&rdquo; I will discuss. The second old problem is distributed hypothesis testing, studied in the late 1980s by Tsitstiklis and others under the name  &ldquo;decentralized detection&rdquo;. Here too, the asymptotic regime was well-studied but the sample complexity question remained unaddressed. Our analyses contain several technical results, such as new f-divergence inequalities, reverse Markov inequality, reverse data-processing inequality, and others that may be of independent interest. This talk is based on joint work with Ankit Pensia and Po-Ling Loh.</p>
<h2>Bio</h2>
<p>Varun Jog is Professor of Information Theory and Statistics in the Department of Pure Mathematics and Mathematical Statistics (DPMMS) at the University of Cambridge. Previously, he was an Assistant Professor at UW-Madison (2016-2020) and at Cambridge (2021-2024). He received his B.Tech. degree in Electrical Engineering from IIT Bombay in 2010 and his Ph.D. in Electrical Engineering and Computer Sciences (EECS) from UC Berkeley in 2015. His research interests are in information theory, statistics, and theoretical machine learning. He is a recipient of the NSF-CAREER Award (2020), the R. Narasimhan Memorial Lecture Award (2020), the Eli Jury Award from the EECS Department at UC Berkeley (2015), and the Jack Keil Wolf student paper award at ISIT 2015.</p>
<div id="footer">
<div id="footer-text">
Page generated 2024-10-23 21:56:59 UTC, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
