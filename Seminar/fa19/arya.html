<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Arya Mazumdar (UMass)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="../index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="../past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Arya Mazumdar (UMass)</h1>
</div>
<p>Sep 18, 2019</p>
<h2>Title and Abstract</h2>
<p><b>Sample complexity of mixture of sparse linear regressions (and what does it have to do with the deletion channel?)</b> <br />
<br />
In the problem of learning mixtures of linear regressions, the goal is to learn a collection of signal vectors from a sequence of (possibly noisy) linear measurements, where each measurement is evaluated on an unknown signal drawn uniformly from this collection. This setting is quite expressive and has been studied both in terms of practical applications and for the sake of establishing theoretical guarantees. In this paper, we consider the case where the signal vectors are sparse; this generalizes the popular compressed sensing paradigm. We improve upon the state-of-the-art results as follows: In the noisy case, we resolve a major open question of Yin et al.2019, by showing how to handle collections of more than two vectors and present the first robust reconstruction algorithm, i.e., if the signals are not perfectly sparse, we still learn a good sparse approximation of the signals. In the noiseless case, as well as in the noisy case, we show how to circumvent the need for a restrictive assumption required in the previous work.
Our techniques are quite different from those in the previous work: for the noiseless case, we rely on a property of sparse polynomials and for the noisy case, we provide new connections to learning Gaussian mixtures and use ideas from the theory of error correcting codes.</p>
<p>What does this have to do with the deletion channel? Turns out that a popular recovery problem in the deletion channel, called trace reconstruction, leads to developing tools and techniques that we use in the above project.</p>
<h2>Bio</h2>
<p>Arya Mazumdar is an associate professor in the College of Information and Computer Sciences at the University of Massachusetts Amherst, on leave now at Amazon AI and Search Sciences. Prior to this, Arya was an assistant professor at the University of Minnesota-Twin Cities, and a postdoctoral scholar at Massachusetts Institute of Technology. Arya received his Ph.D. from the University of Maryland, College Park, where his thesis won the Distinguished Dissertation Award. Arya is a recipient of an NSF CAREER award and an IEEE ISIT Jack K. Wolf Paper Award. Arya's research interests include machine learning, coding theory, and information theory</p>
<div id="footer">
<div id="footer-text">
Page generated 2019-10-11 16:39:12 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
