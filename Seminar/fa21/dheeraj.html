<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Dheeraj Nagaraj (Massachusetts Institute of Technology)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Dheeraj Nagaraj (Massachusetts Institute of Technology)</h1>
</div>
<p>Sep 24, 2021</p>
<h2>Title and Abstract</h2>
<p><b>Reverse Experience Replay: A Streaming Method to Learn with Dependent Data</b><br />
<br /></p>
<p>Learning from a single trajectory of Markov processes is an important task with applications to system identification, time series analysis and reinforcement learning, where it is essential to learn on the go with streaming algorithms. With non-linear system identification and Q-learning in mind, we will first discuss why naively applying methods which work well for i.i.d. data can produce poor behavior in these settings by getting coupled to the underlying Markov process. We will then describe a novel modification called reverse experience replay (RER) &ndash; a rigorous form of experience replay (ER) &ndash; to efficiently unwind these spurious dependencies and obtain near optimal learning algorithms. </p>
<p>In the second part of the talk, we will concentrate on the application to Q-learning, which is known to suffer from poor convergence properties even in simple cases but is widely used in practice. Incorporating the practically used heuristic called online target learning (OTL) and RER with Q-learning, we obtain novel variants with better convergence properties. These variants, unlike vanilla Q learning, globally converge with linear function approximation (under inherent Bellman error conditions) and some are near minimax optimal in the tabular setting. </p>
<h2>Bio</h2>
<p>Dheeraj Nagaraj is a sixth year graduate student at Lab for Information and Decision Systems (MIT), advised by Prof. Guy Bresler. His research focuses on various topics in theoretical machine learning, including stochastic optimization, applied probability, reinforcement learning and neural networks.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-03-17 10:19:07 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
