<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Ashwin Pananjady (Georgia Institute of Technology)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ashwin Pananjady (Georgia Institute of Technology)</h1>
</div>
<p>Nov 5, 2021</p>
<h2>Title and Abstract</h2>
<p><b>Sharp convergence guarantees for iterative algorithms in random optimization problems</b><br />
<br /></p>
<p>Iterative algorithms are the workhorses of modern statistical learning, and are widely used to fit large-scale, complex models to random data. While the choice of an algorithm and its hyperparameters determines both the speed and fidelity of the learning pipeline, it is common for this choice to be made heuristically, either by expensive trial-and-error or by comparing rough bounds on convergence rates of various candidate algorithms. Motivated by this, we develop a principled framework that produces sharp, iterate-by-iterate characterizations of solution quality for algorithms run with sample-splitting on a wide range of nonconvex model-fitting problems with Gaussian data. I will present the general framework and use it to derive several concrete consequences for parameter estimation in some popular statistical models, covering both higher-order algorithms based on alternating updates and first-order algorithms based on subgradient descent. These corollaries in turn facilitate rigorous comparisons between algorithms and reveal several nonstandard statistical and computational phenomena.</p>
<h2>Bio</h2>
<p>Ashwin Pananjady is an Assistant Professor at Georgia Tech, with a joint appointment between the Schools of ISyE and ECE. His research currently focuses on the interplay of statistics and optimization in nonparametric ranking, nonconvex model-fitting, and reinforcement learning. He is a recipient of the Lawrence Brown Award from the Institute of Mathematical Statistics and the David Sakrison Memorial Prize from the EECS department at Berkeley, where his most significant accomplishment was to convince Martin Wainwright to walk up the hill to Cory Hall instead of taking the bus</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-01-07 22:42:50 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
