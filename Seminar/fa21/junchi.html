<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Junchi Li (Berkeley)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Junchi Li (Berkeley)</h1>
</div>
<p>Nov 19, 2021</p>
<h2>Title and Abstract</h2>
<p><b>Some Recent Progress in Nonconvex and Statistical Optimization</b><br />
<br /></p>
<p>I will talk about some of our recent progress in nonconvex and statistical optimization. In the first part, I will introduce a new convergence analysis for an online tensorial independent component analysis (ICA) algorithm &#8201;&mdash;&#8201; a popular dimension reduction tool in statistical machine learning and signal processing &#8201;&mdash;&#8201; by viewing the problem as a nonconvex stochastic approximation problem. For estimating one component, we provide a dynamics-based analysis to prove that our online tensorial ICA algorithm with a specific choice of stepsize achieves a sharp finite-sample error bound. In particular, under a mild assumption on the data-generating distribution and a scaling condition such that \(d^4/T\) is sufficiently small up to a polylogarithmic factor of data dimension \(d\) and sample size \(T\), a sharp finite-sample error bound of \(\tilde{O}(\sqrt{d/T})\) can be obtained. In the second part, I will present a new single algorithm that simultaneously achieves sharp nonasymptotic convergence rates and asymptotic efficiency. We consider first-order stochastic optimization from a general statistical point of view, motivating a specific form of recursive averaging of past stochastic gradients. The resulting algorithm, which we refer to as Recursive One-Over-T SGD (ROOT-SGD), matches the state-of-the-art convergence rate among online stochastic approximation methods. Equipped with a novel multi-epoch design, the last-iterate of ROOT-SGD achieves an optimal statistical risk with a near-unity prefactor both nonasymptotically and asymptotically. Time permitting, I will also briefly talk about some recent results on (stochastic) dynamics that achieve optimal convergence to game-theoretic equilibria.</p>
<h2>Bio</h2>
<p>Dr. Junchi Li &ldquo;Chris&rdquo; is currently a visiting scientist at the University of California, Berkeley, hosted by Professor Michael I. Jordan. He obtained his B.S. from Peking University, Ph.D. from Duke University, and held visiting roles at Princeton University and Tencent Technology. His research interests include statistical machine learning and optimization, scalable online algorithms for big data analytics, and stochastic dynamics on graphs and social networks. He has published original research articles in both top optimization journals and top machine learning conferences.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-01-25 14:48:09 IST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
