<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>BLISS Seminar</title>
<!-- MathJax -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>BLISS Seminar</h1>
</div>
<h2>About</h2>
<p>The BLISS seminar (formerly <a href="http://www-networking.eecs.berkeley.edu/Seminar/" target=&ldquo;blank&rdquo;>NCD seminar</a>) 

is the area seminar of the <a href="http://bliss.eecs.berkeley.edu" target=&ldquo;blank&rdquo;>Berkeley Laboratory for Information and System Sciences</a>. Talks at the seminar cover topics including but not limited to information and coding theory, signal processing, optimization, statistics, and control. The list of talks for the current semester can be found below, and past seminars from 2016 onwards are listed <a href="past.html" target=&ldquo;blank&rdquo;>here</a>. For an archive of all talks from 1996-2015, visit the <a href="http://www-networking.eecs.berkeley.edu/Seminar/" target=&ldquo;blank&rdquo;>old webpage.</a></p>
<p>A calendar of all the talks is maintained <a href="https://calendar.google.com/calendar/b/1?cid=YmVya2VsZXkuZWR1X2Fub2JqY2JqMjJ2Z2M2M3F0cXJtZWowN21jQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20" target=&ldquo;blank&rdquo;>here</a>. Feel free to add it to your own.</p>
<div class="infoblock">
<div class="blockcontent">
<p><b>Fall 2022</b> <br />
Location: Virtual (subscribe to our <a href="https://lists.eecs.berkeley.edu/sympa/subscribe/bliss-seminar" target=&ldquo;blank&rdquo;>mailing list</a> for details) <br />
Regular seminar time: <b>Wednesdays 11 AM - 12 PM PT</b> <br />
Regular seminar venue: <b>Hughes Conference Room, 400 Cory Hall, Berkeley</b> (<a href="https://goo.gl/maps/2QyKkPSB1Nr5gCHW6" target=&ldquo;blank&rdquo;>map</a>)</p>
<p>To subscribe to our mailing list, click <a href="https://lists.eecs.berkeley.edu/sympa/subscribe/bliss-seminar" target=&ldquo;blank&rdquo;>here</a>. <br /></p>
<p>To give a talk at the seminar, contact <a href="https://people.eecs.berkeley.edu/~nived.rajaraman" target=&ldquo;blank&rdquo;>Nived Rajaraman</a> or <a href="http://www.eecs.berkeley.edu/~courtade" target=&ldquo;blank&rdquo;>Tom Courtade</a>.</p>
</div></div>
<h2>Spring 2023 Schedule</h2>
<p>Dates marked in bold indicate that talks are at non-regular dates / times.</p>
<table id="tlayout">
<tr class="r1"><td class="c1">March 1 </td><td class="c2"> Ananya Kumar (Stanford) </td><td class="c3"> Foundation Models for Robustness to Distribution Shifts </td><td class="c4"> <a href="./fa22/ananya.html" target=&ldquo;blank&rdquo;>details</a> </td></tr>
<tr class="r2"><td class="c1">March 15 </td><td class="c2"> Ayush Sekhari (MIT) </td><td class="c3">  </td><td class="c4"> <a href="./fa22/ayush.html" target=&ldquo;blank&rdquo;>details</a> </td></tr>
<tr class="r3"><td class="c1">March 22 </td><td class="c2"> Venugopal Veeravalli (UIUC) </td><td class="c3">  </td><td class="c4"> <a href="./fa22/venugopal.html" target=&ldquo;blank&rdquo;>details</a> </td></tr>
<tr class="r4"><td class="c1">
</td></tr></table>
<p>Abstract: When ML systems are deployed, they often face test examples that are different from training&#8201;&mdash;&#8201;this leads to a large drop in accuracy. The foundation model paradigm (pretraining general-purpose representations from broad unlabeled data, and then adapting to a variety of tasks we care about) has emerged as one of the most effective ways to improve robustness to novel test examples. But how should we adapt good foundation models robustly, and how should we pretrain good models? (1, Adaptation) In the first part of the talk, we will explain why the standard approach of fine-tuning all model parameters can distort good pretrained representations and underperform out-of-distribution. The theory leads to practical insights and better methods for fine-tuning. Our methods have led to state-of-the-art accuracies on ImageNet and in applications such as satellite remote sensing, wildlife conservation, and radiology. (2, Pretraining) Next, we will examine how foundation models can learn good representations. We show that contrastive pretraining on unlabeled data from many domains, and then transferring to labeled data from one domain, improves accuracy even on the domains where we had no labels. We explain why pretraining can work differently from some classical domain adaptation intuitions. Our theory predicts phenomena on real datasets, and leads to improved algorithms. (3, Future Work) Finally, we discuss some exciting future research directions on foundation models.</p>
<p>Bio: Ananya is a final year PhD student at Stanford University advised by Percy Liang and Tengyu Ma. His PhD work focuses on representation learning, foundation models, and reliable machine learning</p>
<div id="footer">
<div id="footer-text">
Page generated 2023-02-26 11:14:33 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
