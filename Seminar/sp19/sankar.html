<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Lalitha Sankar (Arizona State)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="../index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="../past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Lalitha Sankar (Arizona State)</h1>
</div>
<p>March 11.</p>
<h2>Title and Abstract</h2>
<p><b>Information-theoretic Privacy: Leakage measures, robust privacy guarantees, and generative adversarial mechanism design</b> <br />
Privacy is the problem of ensuring limited leakage of information about sensitive features while
sharing information (utility) about non-private features to legitimate data users. Even as
differential privacy has emerged as a strong desideratum for privacy, there is also an equally strong
need for context-aware utility-guaranteeing approaches in most data sharing settings. This talk
approaches this dual requirement using an information-theoretic approach that includes
operationally motivated leakage measures, design of privacy mechanisms, and verifiable
implementations using generative adversarial models. Specifically, we introduce maximal alpha
leakage as a new class of adversarially motivated tunable leakage measures based on accurately
guessing an arbitrary function of a dataset conditioned on a released dataset. The choice of alpha
determines the specific adversarial action ranging from refining a belief for alpha = 1 to guessing
the best posterior for alpha = âˆž, and for these extremal values this measure simplifies to mutual
information (MI) and maximal leakage (MaxL), respectively. The problem of guaranteeing
privacy can then be viewed as one of designing a randomizing mechanism that minimizes
(maximal) alpha leakage subject to utility constraints. We then present bounds on the robustness
of privacy guarantees that can be made when designing mechanisms from a finite number of
samples. Finally, we focus on a data-driven approach, generative adversarial privacy (GAP), to
design privacy mechanisms using neural networks. GAP is modeled as a constrained minimax
game between a privatizer (intent on publishing a utility-guaranteeing learning representation that
limits leakage of the sensitive features) and an adversary (intent on learning the sensitive features).
We demonstrate the performance of GAP on multi-dimensional Gaussian mixture models and the
GENKI dataset. Time permitting, we will briefly discuss the learning-theoretic underpinnings of
GAP as well as connections to the problem of algorithmic fairness.
This work is a result of multiple collaborations: (a) maximal alpha leakage with J. Liao (ASU),
O. Kosut (ASU), and F. P. Calmon (Harvard); (b) robust mechanism design with M. Diaz
(ASU), H. Wang (Harvard), and F. P. Calmon (Harvard); and (c) GAP with C. Huang (ASU), P.
Kairouz (Google), X. Chen (Stanford), and R. Rajagopal (Stanford).</p>
<h2>Bio</h2>
<p>Lalitha Sankar is an Associate Professor in the School of Electrical, Computer, and Energy
Engineering at Arizona State University. Prior to this, she was an Associate Research Scholar at
Princeton University. Sankar was a recipient of a three year Science and Technology Teaching
Postdoctoral Fellowship from the Council on Science and Technology at Princeton University.
Prior to her doctoral studies, she was a Senior Member of Technical Staff at AT&amp;T Shannon
Laboratories. She received the B.Tech degree from the Indian Institute of Technology, Bombay,
the M.S. degree from the University of Maryland, and the Ph.D degree from Rutgers University.
Her research interests include applying information and learning theoretic methods to a variety of
problems including privacy and cyber-security. She received the NSF CAREER award in 2014.
She received the IEEE Globecom 2011 Best Paper Award for her work on privacy of side-
information in multi-user data systems. For her doctoral work, she received the 2007-2008
Electrical Engineering Academic Achievement Award from Rutgers University</p>
<div id="footer">
<div id="footer-text">
Page generated 2019-03-02 17:21:54 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
