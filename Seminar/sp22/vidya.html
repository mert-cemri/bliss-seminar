<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Vidya Muthukumar (Georgia Tech)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Vidya Muthukumar (Georgia Tech)</h1>
</div>
<p>Apr 13, 2022</p>
<h2>Title and Abstract</h2>
<p><b>Classification versus regression in overparameterized regimes: Does the loss function matter?</b><br />
<br /></p>
<p>While the initial mathematical explanations for “benign overfitting” were provided for regression, almost all success stories of modern machine learning have occurred in classification tasks. In this talk, I will compare classification and regression tasks in the overparameterized linear model (in both noiseless and noisy settings) and present the following results:
On the side of optimization, the minimum-norm interpolating solution is identical to the hard-margin SVM solution either under sufficient effective overparameterization or the “neural collapse” conditions. For multiclass classification, we show an equivalence between two different SVM formulations (the multiclass SVM and the one-versus-all SVM) and interpolation. Coupled with characterizations of the implicit bias of gradient descent, our results imply that training with the cross-entropy loss and squared loss yield exactly identical solutions.
On the side of generalization, we uncover high-dimensional regimes where the minimum-norm interpolating solution generalizes well for a classification task, but does not generalize in a corresponding regression task. We show that classification generalization is possible despite adverse signal recovery and in regimes where margin-based bounds are not predictive of generalization.
These results show the contrasting roles of training and test loss functions in the overparameterized regime.
Time permitting, I will conclude the talk with partial extensions of these results to kernel interpolation, and a brief discussion of consequences for adversarial robustness.</p>
<h2>Bio</h2>
<p>Vidya Muthukumar is an Assistant Professor in the Schools of Electrical and Computer Engineering and Industrial and Systems Engineering at Georgia Institute of Technology. Her broad interests are in game theory, online and statistical learning. She is particularly interested in designing online learning algorithms that provably adapt in strategic environments, fundamental properties of overparameterized models, and the intersection of game theory and reinforcement learning.</p>
<p>Vidya received the PhD degree in Electrical Engineering and Computer Sciences from University of California, Berkeley. She is the recipient of an Adobe Data Science Research Award, Simons-Berkeley Research Fellowship (for the Fall 2020 program on "Theory of Reinforcement Learning”), and a Georgia Tech Class of 1969 Teaching Fellowship for the academic year 2021-2022.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-08-25 14:03:41 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
