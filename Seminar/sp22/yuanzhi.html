<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yuanzhi Li (CMU)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yuanzhi Li (CMU)</h1>
</div>
<p>Apr 6, 2022</p>
<h2>Title and Abstract</h2>
<p><b>Deep learning when the data set has multiple features</b><br />
<br /></p>
<p>Standard (supervised learning) data set typically contains multiple features that we can use to classify the label correctly. For example, we can distinguish a car from a cat by looking at the light of the car, the window, or the wheels etc. In this talk, we will discuss how deep learning can perform fundamentally different compared to linear models (including NTKs) on data sets with multiple features. We develop a new theorem framework on top of that to explain many intricating behaviors in deep learning, such as ensemble, knowledge distillation, and self-distillation; The role of the prediction head in self-supervised learning, and how linear mix-up data augmentation works for non-linearly separable data.</p>
<h2>Bio</h2>
<p>Yuanzhi Li is an assistant professor at CMU, Machine Learning Department, and a visiting researcher at Microsoft. His primary research area is deep learning theory, focusing on understanding the (hierarchical) feature learning process in neural networks, how its better than shallow learning methods and how its influenced by the choice of optimization algorithms. He did his Ph.D. at Princeton under the advise of Sanjeev Arora.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-08-25 14:09:21 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
