# jemdoc: menu{MENU}{index.html}
= BLISS Seminar

== About
The BLISS seminar (formerly [http://www-networking.eecs.berkeley.edu/Seminar/ NCD seminar]) 
#is co-sponsored by generous grants from [https://www.microsoft.com/en-us/research/ Microsoft Research] and [www.qualcomm.com Qualcomm], and 
is the area seminar of the [http://bliss.eecs.berkeley.edu Berkeley Laboratory for Information and System Sciences]. Talks at the seminar cover topics including but not limited to information and coding theory, signal processing, optimization, statistics, and control. The list of talks for the current semester can be found below, and past seminars from 2016 onwards are listed [past.html here]. For an archive of all talks from 1996-2015, visit the [http://www-networking.eecs.berkeley.edu/Seminar/ old webpage.]

A calendar of all the talks is maintained [https://calendar.google.com/calendar/b/1?cid=YmVya2VsZXkuZWR1X2Fub2JqY2JqMjJ2Z2M2M3F0cXJtZWowN21jQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20 here]. Feel free to add it to your own.

~~~
*Fall 2022* \n
Location: Virtual (subscribe to our [https://lists.eecs.berkeley.edu/sympa/subscribe/bliss-seminar mailing list] for details) \n
Regular seminar time: *Wednesdays 11 AM - 12 PM PT* \n
Regular seminar venue: *Hughes Conference Room, 400 Cory Hall, Berkeley* ([https://goo.gl/maps/2QyKkPSB1Nr5gCHW6 map])

To subscribe to our mailing list, click [https://lists.eecs.berkeley.edu/sympa/subscribe/bliss-seminar here]. \n

To give a talk at the seminar, contact [https://people.eecs.berkeley.edu/~nived.rajaraman Nived Rajaraman] or [http://www.eecs.berkeley.edu/~courtade Tom Courtade].
~~~


== Spring 2023 Schedule

Dates marked in bold indicate that talks are at non-regular dates / times.
~~~
{}{table}{tlayout}
March 1 | Ananya Kumar (Stanford) | Foundation Models for Robustness to Distribution Shifts | [./fa22/ananya.html details] ||
March 15 | Ayush Sekhari (MIT) |  | [./fa22/ayush.html details] ||
March 22 | Venugopal Veeravalli (UIUC) |  | [./fa22/venugopal.html details] ||
~~~



Abstract: When ML systems are deployed, they often face test examples that are different from training---this leads to a large drop in accuracy. The foundation model paradigm (pretraining general-purpose representations from broad unlabeled data, and then adapting to a variety of tasks we care about) has emerged as one of the most effective ways to improve robustness to novel test examples. But how should we adapt good foundation models robustly, and how should we pretrain good models? (1, Adaptation) In the first part of the talk, we will explain why the standard approach of fine-tuning all model parameters can distort good pretrained representations and underperform out-of-distribution. The theory leads to practical insights and better methods for fine-tuning. Our methods have led to state-of-the-art accuracies on ImageNet and in applications such as satellite remote sensing, wildlife conservation, and radiology. (2, Pretraining) Next, we will examine how foundation models can learn good representations. We show that contrastive pretraining on unlabeled data from many domains, and then transferring to labeled data from one domain, improves accuracy even on the domains where we had no labels. We explain why pretraining can work differently from some classical domain adaptation intuitions. Our theory predicts phenomena on real datasets, and leads to improved algorithms. (3, Future Work) Finally, we discuss some exciting future research directions on foundation models.

Bio: Ananya is a final year PhD student at Stanford University advised by Percy Liang and Tengyu Ma. His PhD work focuses on representation learning, foundation models, and reliable machine learning.