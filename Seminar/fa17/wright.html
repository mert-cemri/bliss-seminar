<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Stephen Wright (UW Madison)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Stephen Wright (UW Madison)</h1>
</div>
<p>Sep 25, 2017.</p>
<h2>Title and Abstract</h2>
<p><b>Algorithmic Tools for Smooth Nonconvex Optimization</b> <br /></p>
<p>Unconstrained optimization of a smooth nonconvex objective over many
variables is a classic problem in optimization. Several effective
techniques have been proposed over the years, along with results about
global and local convergence. There has been an upsurge of interest
recently on techniques with good global complexity properties. (This
interest is being driven largely by researchers in machine learning,
who want to solve the nonconvex problems arising from neural network
training and robust statistics, but it has roots in the optimization
literature.) In this talk we describe the algorithmic tools that can
be used to design methods with appealing practical behavior as well as
provably good global convergence properties. These tools include the
conjugate gradient and Lanczos algorithms, Newton's method, cubic
regularization, trust regions, and accelerated gradient. We show how
these elements can be assembled into a comprehensive method, and
compare a number of proposals that have been made to date. If time
permits, we will consider the behavior of first-order methods in the
vicinity of saddle points, showing that accelerated gradient methods
are as unlikely as gradient descent to converge to saddle points, and
escapes from such points faster.  </p>
<p>This talk presents joint work with Clement Royer and Mike O'Neill
(both of U Wisconsin-Madison).</p>
<h2>Bio</h2>
<p>Stephen J. Wright holds the George B. Dantzig Professorship, the
Sheldon Lubar Chair, and the Amar and Balinder Sohi Professorship of
Computer Sciences at the University of Wisconsin-Madison. His research
is in computational optimization and its applications to many areas of
science and engineering. Prior to joining UW-Madison in 2001, Wright
held positions at North Carolina State University (1986-90), Argonne
National Laboratory (1990-2001), and  the University of
Chicago (2000-2001). He has served as Chair of the Mathematical
Optimization Society and as a Trustee of SIAM. He is a Fellow of
SIAM. In 2014, he won the W.R.G. Baker award from IEEE.</p>
<p>Wright is the author / coauthor of widely used text and reference
books in optimization including &ldquo;Primal Dual Interior-Point Methods&rdquo;
and &ldquo;Numerical Optimization&rdquo;. He has published widely on optimization
theory, algorithms, software, and applications.</p>
<p>Wright is current editor-in-chief of the SIAM Journal on Optimization
and previously served as editor-in-chief or associate editor of
Mathematical Programming (Series A), Mathematical Programming (Series
B), SIAM Review, SIAM Journal on Scientific Computing, and several
other journals and book series</p>
<div id="footer">
<div id="footer-text">
Page generated 2017-09-14 10:32:13 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
