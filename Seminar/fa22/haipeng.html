<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Haipeng Luo (University of Southern California)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Haipeng Luo (University of Southern California)</h1>
</div>
<p>August 29, 2022</p>
<h2>Title and Abstract</h2>
<p><b>Near-Optimal No-Regret Learning for General Convex Games &#8201;&mdash;&#8201; The Role of Positive Regret
</b><br />
<br /></p>
<p>A recent line of work has established uncoupled learning dynamics such that, when employed by all players in a game, each playerâ€™s regret after T repetitions grows polylogarithmically in T, an exponential improvement over the traditional guarantees within the no-regret framework. However, so far these results have only been limited to certain classes of games with structured strategy spaces, such as normal-form and extensive-form games. In this work, we significantly simplify, improve, and extend these results, showing that near-optimal O(log T) per-player regret is achievable for general convex games via a new and efficient learning dynamic. The key idea of our approach starts from a simple observation that it suffices to control the positive part of the regret, which we then show can be converted to the standard regret of a modified problem in a lifted space. </p>
<p>This talk is based on the following two joint works with Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, and Tuomas Sandholm:<br />
<a href="https://arxiv.org/pdf/2204.11417.pdf" target=&ldquo;blank&rdquo;>https://arxiv.org/pdf/2204.11417.pdf</a><br />
<a href="https://arxiv.org/pdf/2206.08742.pdf" target=&ldquo;blank&rdquo;>https://arxiv.org/pdf/2206.08742.pdf</a></p>
<h2>Bio</h2>
<p>Haipeng Luo is an assistant professor in the Department of Computer Science at the University of Southern California. He obtained his PhD from Princeton University in 2016 and spent a year at Microsoft Research, NYC as a post-doc researcher afterwards. His research interest is in developing practical machine learning algorithms with strong theoretical guarantees, with a focus on online learning, bandit algorithms, reinforcement learning, learning in games, and others. He has received several awards over the years, including NSF CAREER award, NSF CRII award, Google Faculty Research Award, Google Research Scholar Award, Best Paper Awards at ICML&rsquo;15, NeurIPS&rsquo;15, and COLT&rsquo;21, and Best Student Paper Award at COLT&rsquo;18</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-10-04 10:56:32 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
