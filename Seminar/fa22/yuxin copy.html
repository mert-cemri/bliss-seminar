<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yuxin Chen (University of Pennsylvania)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yuxin Chen (University of Pennsylvania)</h1>
</div>
<p>October 10, 2022</p>
<h2>Title and Abstract</h2>
<p><b>Towards Optimal Sample Complexities in Offline Reinforcement Learning and Markov Games</b><br />
<br /></p>
<p>Emerging reinforcement learning (RL) applications necessitate the design of sample-efficient solutions in order to accommodate the explosive growth of problem dimensionality. Despite the empirical success, however, our understanding about the statistical limits of RL remains highly incomplete. In this talk, I will present some recent progress towards settling the sample complexity in two RL scenarios. The first one is concerned with offline or batch RL, which performs learning using only pre-collected data without further exploration. We prove that model-based offline RL &#8201;&mdash;&#8201; a plug-in approach that leverages the pessimism principle with Bernstein-style penalty &#8201;&mdash;&#8201; achieves minimal-optimal sample complexity without any burn-in cost. The second scenario is concerned with multi-agent RL in zero-sum Markov games, assuming access to a generative model (a.k.a. simulator). We develop a new algorithm &#8201;&mdash;&#8201; built upon the integration of adaptive sampling, online learning, and the optimism principle &#8201;&mdash;&#8201; that overcomes the curse of multi-agents and the barrier of long horizon simultaneously.  Our results emphasize the prolific interplay between high-dimensional statistics, online learning, and game theory. (See https:<i></i>arxiv.org<i>abs</i>2204.05275 and https:<i></i>arxiv.org<i>abs</i>2208.10458 for more details).</p>
<p>This is based on joint work with Gen Li, Laixi Shi, Yuling Yan, Yuejie Chi, Jianqing Fan, and Yuting Wei. </p>
<h2>Bio</h2>
<p>Yuxin Chen is currently an associate professor in the Department of Statistics and Data Science at the University of Pennsylvania. Before joining UPenn, he was an assistant professor of electrical and computer engineering at Princeton University. He completed his Ph.D. in Electrical Engineering at Stanford University, and was also a postdoc scholar at Stanford Statistics. His current research interests include high-dimensional statistics, nonconvex optimization, and reinforcement learning. He has received the Alfred P. Sloan Research Fellowship, the ICCM best paper award (gold medal), the AFOSR and ARO Young Investigator Awards, the Google Research Scholar Award, and was selected as a finalist for the Best Paper Prize for Young Researchers in Continuous Optimization. He has also received the Princeton Graduate Mentoring Award.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-10-30 11:24:14 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
