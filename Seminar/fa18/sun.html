<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Ruoyu Sun (UIUC)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ruoyu Sun (UIUC)</h1>
</div>
<p>Oct 15.</p>
<h2>Title and Abstract</h2>
<p><b>Understanding the Landscape of Neural Networks for Binary Classification</b> <br />
One challenge of training neural networks is the non-convexity of the loss function, which can lead to many bad local minima. Due to the recent success of neural networks, it is conjectured that all local minima lead to similar training performance and are thus not a big issue. In this talk, instead of explaining &ldquo;why neural-net landscape is nice&rdquo;, we try to understand &ldquo;when the landscape is nice, and when is not&rdquo;. We will focus on the binary classification problem, and provide both positive and negative results. On the positive side, we prove that no bad local minima exist under a few conditions on the neuron types, the neural network structure, the loss function, and the dataset. On the negative side, we provide dozens of counterexamples which show that bad local minima exist when these conditions do not hold.</p>
<h2>Bio</h2>
<p>Ruoyu Sun is an assistant professor in the Department of Industrial and Enterprise Systems Engineering Department (ISE) and Coordinate Science Lab (CSL), University of Illinois at Urbana-Champaign. Before joining UIUC, he was a visiting research scientist at Facebook AI Research and was a postdoctoral researcher at Stanford University. He obtained PhD in electrical engineering from University of Minnesota, and B.S. in mathematics from Peking University. He has won the second place of INFORMS George Nicholson student paper competition, and honorable mention of INFORMS optimization society student paper competition. His research interests lie in optimization and machine learning, especially large-scale optimization and non-convex optimization for machine learning</p>
<div id="footer">
<div id="footer-text">
Page generated 2018-10-10 16:26:03 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
