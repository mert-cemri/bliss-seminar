<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Sam Hopkins (UC Berkeley)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Sam Hopkins (UC Berkeley)</h1>
</div>
<p>Sep 11, 2020</p>
<h2>Title and Abstract</h2>
<p><b>Recent Advances in Algorithmic Heavy-Tailed Statistics</b><br />
<br />
Recent work in high-dimensional statistics shows that for many classical problems (mean estimation, covariance estimation, linear regression, sparse recovery&hellip;) it is possible to match the confidence intervals obtained by classical estimators (empirical mean/covariance, ordinary least squares, lasso&hellip;) in the case of Gaussian data but making much weaker assumptions, by using more robust high-dimensional estimators. Typically, one can replace a (sub)-Gaussian assumption with an assumption on 2 moments of the underlying distribution but obtain the same non-asymptotic, finite-sample guarantees!</p>
<p>However, most of the high-dimensional estimators known to obtain such sharp confidence intervals under weak assumptions appear computationally intractable â€” requiring running time exponential in dimension. I will discuss several recent works giving polynomial-time algorithms with similar guarantees, focusing on new algorithms for high-confidence covariance estimation and linear regression in joint work with Cherapanamjeri, Kathuria, Raghavendra, and Tripuraneni, STOC 2020.</p>
<h2>Bio</h2>
<p>Sam Hopkins is a Miller Postdoctoral Fellow at UC Berkeley in the department of Electrical Engineering and Computer Science. He works on algorithms and computational complexity, especially for computationally challenging problems in high-dimensional statistics. He obtained a PhD from Cornell University, where his work was supported by a Microsoft Graduate Fellowship and an NSF Graduate Research Fellowship, and a BS from the University of Washington. In 2021 he will join MIT as an assistant professor of computer science</p>
<div id="footer">
<div id="footer-text">
Page generated 2020-10-16 16:11:33 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
