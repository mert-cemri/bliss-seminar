<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Salman Avastimehr (USC)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Salman Avastimehr (USC)</h1>
</div>
<p>Apr 23, 2021</p>
<h2>Title and Abstract</h2>
<p><b>Trustworthy and Scalable Federated Learning</b><br />
<br /></p>
<p>Federated learning (FL) is a promising framework for enabling privacy preserving machine learning across many decentralized users. Its key idea is to leverage local training at each user without the need for centralizing/moving any device's dataset in order to protect users’ privacy. In this talk, I will highlight several exciting research challenges for making such a decentralized system trustworthy and scalable to a large number of resource-constrained users. In particular, I will discuss three directions: (1) resilient and secure model aggregation, which is a key component and performance bottleneck in FL; (2) FL of large models, via knowledge transfer, over resource-constrained users; and (3) FedML, our open-source research library and benchmarking ecosystem for FL research (fedml.ai).</p>
<p>This talk is based on several papers: TurboAggregate (JSAIT’21, arXiv:2002.04156), Byzantine-Resilient Secure Federated Learning (JSAC’20, arXiv:2007.11115), FedGKT (NeurIPS’20, arXiv:2007.14513), FedNAS (CVPR-NAS’20, arXiv:2004.08546), and FedML (NeurIPS-SpicyFL’20, arXiv:2007.13518).</p>
<h2>Bio</h2>
<p>Salman Avestimehr is a Dean's Professor, the inaugural director of the USC-Amazon Center on Secure and Trusted Machine Learning (Trusted AI), and the director of the Information Theory and Machine Learning (vITAL) research lab at the Electrical and Computer Engineering Department of University of Southern California. He is also an Amazon Scholar at Amazon<i>Alexa-AI. He received his Ph.D. in 2008 in Electrical Engineering and Computer Science from the University of California. His research interests include information theory, machine learning, distributed computing, and secure and private learning</i>computing. Dr. Avestimehr has received a number of awards for his research and teaching, including the James L. Massey Research &amp; Teaching Award from IEEE Information Theory Society, an Information Theory Society and Communication Society Joint Paper Award, a Presidential Early Career Award for Scientists and Engineers (PECASE) from the White House (President Obama), a USC Mentoring Award, a Young Investigator Program (YIP) award from the U. S. Air Force Office of Scientific Research, a National Science Foundation CAREER award, the David J. Sakrison Memorial Prize from UC Berkeley EECS Department, and several Best Paper Awards at Conferences. He has been an Associate Editor for IEEE Transactions on Information Theory and a general Co-Chair of the 2020 International Symposium on Information Theory (ISIT). He is a fellow of IEEE</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-10-11 14:15:17 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
