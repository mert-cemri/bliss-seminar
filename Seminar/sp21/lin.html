<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Lin F. Yang (UCLA)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Seminar&nbsp;home</a></div>
<div class="menu-item"><a href="past.html">Past&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Lin F. Yang (UCLA)</h1>
</div>
<p>Feb 19, 2021</p>
<h2>Title and Abstract</h2>
<p><b>Efficient methods for reinforcement learning with general function approximation</b><br />
<br /></p>
<p>Abstract:
Designing provably efficient algorithms with general function approximation is an important open problem in reinforcement learning. Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of general function approximation schemes largely remains missing. In this talk, we show algorithm designs for both value-based and policy-based RL with general function approximation. Firstly, by applying an novel online sub-sampling technique, we first establish a novel value-based algorithm that takes poly(dH) computation time per round on average and enjoys a regret bound &nbsp;poly(H)sqrt(T), where d depends on the complexity of the function class, independent of the size of the state space. Furthermore, the algorithm achieves low switching cost, i.e., it changes the policy only poly(dH) times during the entire execution, making it appealing to be implemented in real-life scenarios. Moreover, by using an upper-confidence-based exploration-driven reward function, the algorithm provably explores the environment in the reward-free setting (i.e., unsupervised RL). Using the same function class, we then design policy-gradient-based methods that achieve near-optimal policy within poly(dH) number of explorations. Empirical studies show that these theoretically principled methods outperform the baselines consistently.</p>
<h2>Bio</h2>
<p>Dr. Lin Yang is an assistant professor in the Electrical and Computer Engineering Department at the University of California, Los Angeles. His current research focus is on reinforcement learning theory and applications, learning for control, non-convex optimization, and streaming algorithms. Previously, he was a postdoc at Princeton University. He obtained two Ph.D. degrees (in Computer Science and in Physics &amp; Astronomy) simultaneously, from Johns Hopkins University. Prior to that, he obtained a Bachelor's degree in Math &amp; Physics from Tsinghua University. He was a recipient of the Simons-Berkeley Research Fellowship, JHU MINDS best dissertation award, and the Dean Robert H. Roy Fellowship</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-09-13 23:02:05 PDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
